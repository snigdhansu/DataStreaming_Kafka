# Github Trending Repositories

## Architecture

![Flink-weather-streaming](link for the architecture image)

## Overview

The project simulates continuously reads weather data from a Kafka topic, calculates the average temperature for each city every minute, and stores the results in a PostgreSQL database.

Here's a detailed breakdown of the project:

**1. Kafka Producer:**

* **gen_data()** function requests github event data every 20 seconds and sends it to the 'github-events' topic in Kafka.
* **Language:** Python

**2. Flink Processor:**

* **GithubTrendJob.java** is the main entry point for the Flink application.
* **GithubEventData class** represents a github event data with id, repo, and other attributes.
* **GithubEventDeserializationSchema** is a custom deserialization schema that reads JSON-formatted github event data from Kafka.
* **Main() function** creates a Flink streaming environment and reads event data from the 'github-events' topic using KafkaSource.
* **Aggregation** is performed to sum the number of events based on the repo key
* **SlidingProcessingTimeWindows** is an aggregation function that calculates the count of events for each repo every 2 minutes considering the last 10 minutes data.
* **repoAndValueStream** is a DataStream containing the repo and total events count for each repo.
* **MyProcessWindowFunction** is a custom processing function that sorts the aggregated stream every 2 minutes and retrieves the top 10 repos
* **topTrendingRepos** is a DataStream containing the top 10 repositories with highest number of activities
* **githubSink** is the KafkaSink created to publish the top trending repo information to Kafka cluster topic 'trending-github-repos'
* **Language:** Java

**3. Github Consumer:**

* **KafkaConsumer** consumes the final top 10 trending repos from the topic 'trending-github-repos' 
* **Language:** Python

Overall, the project utilizes Kafka for real-time data ingestion, Flink for data processing and aggregation.

## Instructions for Building and Running a Flink Application with Kafka and PostgreSQL

This document provides comprehensive instructions for building and running a Flink application that ingests github data from Kafka, processes it, to find the trending repositories and sends the results to another Kafka topic for further application usage.

### Prerequisites:

Docker and Docker Compose installed
Familiarity with Flink, Kafka

### Build and Run:

a. Build the Kafka Producer Docker image:
* cd kafka-producer 
* docker build -t kafka-producer .

b. Build the Flink Processor Docker image:
* cd flink-processor
* mvn clean package
* docker build -t flink-processor .

b. Build the Github Consumer Docker image:
* cd github-consumer
* docker build -t github-consumer .

c. Start the Docker containers:
* docker-compose up -d

d. Verify from logs of github-consumer container

Setting up aws for uploading files:

Installations: 
1. pip3 install boto3 
(already in requirements.txt file though)
2. pip3 install awscli
3. aws configure 
(create an account on aws and generate your access key, you will need it for this command)
4. enter your access key and secret key and AWS bucket name in python-consumer.py
5. Build the images and run the code and inside github-consumer container you will see statement that file is uploaded. Open AWS bucket to view/ downlaod the csv file there.



